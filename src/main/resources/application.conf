# Global settings
global {
  #in test mode the input dataset is limited to only part of the entries
  testMode = false
  #weather to include all the issue comments or only its full description
  includeComments = true
  # type of tokenizer to use 0 - dummy | 1 - regexp | 2 - NLP | 3 - POS
  tokenizerType = 3
  # mimimum token length
  minWordSize = 4
  # time intervals to consider
  timeIntervals = 3
  # mimimum number of issues fixed in a time interval in
  # order to consider the user a valid candidate
  issuesThreshold = 3
  # time interval in days in which to apply the issuesThreshold
  timeThreshold = 30
  # use issues updated no later than thin number of days back
  timeThresholdForTraining = 240
}

filesystem {
  root: "/home/aflorea/data/columbugus"
  # root: "gs://dataproc-1e7dadea-6c68-4715-b356-2a8742d25022-eu/columbugus"
}

# Execution phases
phases {
  cleansing = false
  transform = false
  preprocess = false
}

# transform specific params
transform {
  minDocFreq = 2
  maxDocFreq = 1500 # ~15% of the training data size
}

# preprocess specific params
preprocess {
  pca = false
  chi2 = true
  undersampling = false
  normalize = false
  includeCategory = true
  categoryScalingFactor = 92
}

# Spark context
spark {
  master = "local"
  appName = "Columb(ug)us"
  driver.memory = 8g
  client.memory = 8g
}

# MySQL server
mySQL {
  url = "jdbc:mysql://localhost:3306/eclipsebugs"
  username = "root"
  password = "mysql"
}